{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_curve, accuracy_score, roc_auc_score\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('churn_data.csv', sep=';')\n",
        "\n",
        "#new feature: overage fee to avg monthly bill ratio. we can assume lower bill = lower ability to pay or interest in the product. If the overage fee is a higher ratio compared to their bill, they could be disproportionately affected. \n",
        "\n",
        "df['overage_ratio'] = df['OverageFee'] / df['MonthlyCharge']\n",
        "\n",
        "#df_outofdata = df.loc[(df['DataPlan'] == 0) & df['DataUsage'] > 0]\n",
        "\n",
        "# we see there are about 600 people who are using data without a data plan, which I assume has higher fees.\n",
        "\n",
        "df['out_of_plan_data_use'] = (df['DataPlan'] == 0) & (df['DataUsage'] > 0)\n",
        "df['out_of_plan_data_use'] = df['out_of_plan_data_use'].astype(int)\n",
        "\n",
        "#Roaming minutes are charged at higher rates than normal daytime minutes. Let's create another variable showing the ratio of roaming to daytime minutes\n",
        "\n",
        "df['roaming_daytime_ratio'] = df['RoamMins'] / df['DayMins']\n",
        "df['roaming_daytime_ratio'] = df['roaming_daytime_ratio'].replace(np.inf, 1, inplace=False)\n",
        "df.describe()\n",
        "\n",
        "# If the have a high monthly charge, and they have made a lot of calls, then I assume that they would have called to lower the monthly charge. Let's create an interaction term between these two\n",
        "df['cs_calls_bill'] = df['CustServCalls'] * df['MonthlyCharge']\n",
        "\n",
        "#daily call length. If they have longer calls per day, they may be business customers who have different needs than regular customers.\n",
        "#df['daily_call_length'] = (df['DayMins'] / 30.417) * df['DayCalls']\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#TODO scale the distributions of continuous variables (will this make a difference?)\n",
        "#%%\n",
        "#now let's visualize all of our columns and check the distributions\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "cols = list(df.columns)\n",
        "for i in range(len(cols)):\n",
        "    plt.figure(i)\n",
        "    sns.kdeplot(np.array(df.iloc[i]), bw=0.5).set(title=cols[i])\n",
        "\n",
        "\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Split features and target into X and y\n",
        "features = df.drop(labels='Churn', axis=1)\n",
        "X = np.array(features)\n",
        "y = np.array(df['Churn'])\n",
        "y_labels = np.array(['Not churn', 'Churn'])\n",
        "#Create test and train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "# Create a model using DecisionTree classifier\n",
        "clf_tree = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
        "clf_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = clf_tree.predict(X_test)\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict test set labels\n",
        "y_pred = clf_tree.predict(X_test)\n",
        " \n",
        "# Compute test set accuracy  \n",
        "acc_baseline = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set baseline accuracy: {:.3f}\".format(acc_baseline))\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plot the tree\n",
        "plt.figure(figsize=(25,10))\n",
        "\n",
        "#This should work but it doesn't, wtf? \n",
        "#class_names = ['Not_churned', 'Churned']\n",
        "plot_tree = plot_tree(clf_tree,\n",
        "                      feature_names=features.columns, \n",
        "#                     class_names=class_names, \n",
        "                      class_names=True,\n",
        "                      filled=True, \n",
        "                      rounded=True, \n",
        "                      fontsize=14)\n",
        "\n",
        "\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED=2345\n",
        " \n",
        "# Instantiate knn\n",
        "knn = KNN(n_neighbors=5)\n",
        " \n",
        "# Instantiate tree\n",
        "tree = DecisionTreeClassifier(min_samples_leaf = 0.13, random_state=SEED)\n",
        " \n",
        "# Define the list classifiers\n",
        "classifiers = [('K Nearest Neighbours', knn), ('Classifier Tree', tree)]\n",
        "\n",
        "\n",
        "#\n",
        "# Evaluate individual classifiers\n",
        "#\n",
        "from sklearn.metrics import accuracy_score\n",
        " \n",
        "# Iterate over the pre-defined list of classifiers\n",
        "for clf_name, clf in classifiers:    \n",
        "  \n",
        "    # Fit clf to the training set\n",
        "    clf.fit(X_train, y_train)    \n",
        "\n",
        "    # Predict y_pred\n",
        "    y_pred = clf.predict(X_test)\n",
        "     \n",
        "    # Calculate accuracy\n",
        "    acc_clf = accuracy_score(y_test, y_pred) \n",
        "    \n",
        "    # Evaluate clf's accuracy on the test set\n",
        "    print('{:s} : {:.3f}'.format(clf_name, acc_clf))\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Now let's check if we can get a higher accuracy rating with ensembling\n",
        "\n",
        "# Import VotingClassifier from sklearn.ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        " \n",
        "# Instantiate a VotingClassifier vc\n",
        "vc = VotingClassifier(estimators=classifiers, voting='soft')     \n",
        " \n",
        "# Fit vc to the training set\n",
        "vc.fit(X_train, y_train)   \n",
        " \n",
        "# Evaluate the test set predictions\n",
        "y_pred = vc.predict(X_test)\n",
        " \n",
        "# Calculate accuracy score\n",
        "acc_voting = accuracy_score(y_test, y_pred)\n",
        "print('Voting Classifier: {:.3f}'.format(acc_voting))\n",
        "#Nope, doesn't seem like this will give us a better result. .858, so slightly better than knn but not enough to make a difference. \n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Bagging\n",
        "# Import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        " \n",
        "# Import BaggingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        " \n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        " \n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, random_state=1)\n",
        "\n",
        "# Fit bc to the training set\n",
        "bc.fit(X_train, y_train)\n",
        " \n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        " \n",
        "# Evaluate acc_test\n",
        "acc_dt_with_bagging = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of bc: {:.3f}'.format(acc_dt_with_bagging)) \n",
        "#Jump from 85.5% to 92.8% with bagging\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pass KNN into bagging\n",
        "# Instantiate knn\n",
        "#already done\n",
        " \n",
        "# Instantiate bc\n",
        "bc = BaggingClassifier(base_estimator=knn, n_estimators=100, random_state=1)\n",
        "\n",
        "# Fit bc to the training set\n",
        "bc.fit(X_train, y_train)\n",
        " \n",
        "# Predict test set labels\n",
        "y_pred = bc.predict(X_test)\n",
        " \n",
        "# Evaluate acc_test\n",
        "acc_knn_with_bagging = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of knn with bagging: {:.3f}'.format(acc_knn_with_bagging))\n",
        "#This gives us a slightly better model than using knn by itself, but still nowhere near as good as using random forest by itself. \n",
        "auc_knn_with_bagging = roc_auc_score(y_test, y_pred)\n",
        "print(\"Test set AUC of knn_with_bagging: {:.3f}\".format(auc_knn_with_bagging))\n",
        "\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate rf\n",
        "rf = RandomForestClassifier(max_depth=12, random_state=0, n_estimators=500)\n",
        "             \n",
        "# Fit rf to the training set    \n",
        "rf.fit(X_train, y_train) \n",
        " \n",
        "# Predict test set labels\n",
        "y_pred = rf.predict(X_test)\n",
        " \n",
        "# Evaluate acc_test\n",
        "acc_rf = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of rf: {:.3f}'.format(acc_rf)) \n",
        "#RF gets 93.5%\n",
        "\n",
        "# Predict the test set labels\n",
        "y_pred = rf.predict(X_test)\n",
        " \n",
        "# Print ROC_AUC score\n",
        "\n",
        "auc_rf = roc_auc_score(y_test, y_pred)\n",
        "print(\"Test set AUC of rf: {:.3f}\".format(auc_rf))\n",
        "#up from 0.741 to 0.811\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Let's rank feature importance now.\n",
        "# Create a pd.Series of features importances\n",
        "importances = pd.Series(data = rf.feature_importances_,\n",
        "                        index = features.columns)\n",
        " \n",
        "# Sort importances\n",
        "importances_sorted = importances.sort_values()\n",
        " \n",
        "# Draw a horizontal barplot of importances_sorted\n",
        "importances_sorted.plot(kind='barh', color='lightgreen')\n",
        "plt.title('Features Importances')\n",
        "plt.show()\n",
        "\n",
        "#DayMins and MonthlyCharge are the most important feature, plus the daily_call_length feature I added. But that might be because daily_call_length is a function of DayMins\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Now let's try using AdaBoost to see if we can improve AUC\n",
        "# Import AdaBoostClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        " \n",
        "ada_reg = AdaBoostClassifier(n_estimators=100)\n",
        "\n",
        "# Fit ada to the training set\n",
        "ada_reg.fit(X_train, y_train)\n",
        " \n",
        "# Compute the probabilities of obtaining the positive class\n",
        "y_pred_ada = ada_reg.predict(X_test)\n",
        "\n",
        "#Print ROC_AUC score\n",
        "auc_ada = roc_auc_score(y_test, y_pred_ada)\n",
        "print(\"Test set AUC of ada: {:.3f}\".format(auc_ada))\n",
        "\n",
        "#Ada brings it from 0.801 to 0.884, nice.\n",
        "\n",
        "\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Now let's try XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1)\n",
        "\n",
        "# Fit ada to the training set\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Compute the probabilities of obtaining the positive class\n",
        "y_pred_xg = xgb_model.predict(X_test)\n",
        "\n",
        "#Print ROC_AUC score\n",
        "\n",
        "auc_xgb = roc_auc_score(y_test, y_pred)\n",
        "print(\"Test set AUC of xgb: {:.3f}\".format(auc_xgb))\n",
        "#XGB worse than ada (.801 to 0.885)\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hyperparameter tuning\n",
        "# Instantiate rf\n",
        "rf_2 = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "             \n",
        "# Fit rf to the training set    \n",
        "rf_2.fit(X_train, y_train) \n",
        " \n",
        "# Predict test set labels\n",
        "y_pred = rf_2.predict(X_test)\n",
        " \n",
        "# Evaluate acc_test\n",
        "acc_test = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of rf with max_depth_2: {:.4f}'.format(acc_test)) \n",
        "\n",
        "#evaluate ROC_AUC score\n",
        "auc_rf_2 = roc_auc_score(y_test, y_pred)\n",
        "print('Test set AUC of rf with max_depth_2 : {:.4f}'.format(auc_rf_2))\n",
        "# Instantiate rf\n",
        "rf_12 = RandomForestClassifier(max_depth=12, random_state=0)\n",
        "             \n",
        "# Fit rf to the training set    \n",
        "rf_12.fit(X_train, y_train) \n",
        " \n",
        "# Predict test set labels\n",
        "y_pred = rf_12.predict(X_test)\n",
        " \n",
        "# Evaluate acc_test\n",
        "acc_test = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy of rf with max_depth_12: {:.4f}'.format(acc_test)) \n",
        "\n",
        "#evaluate ROC_AUC score\n",
        "auc_rf_12 = roc_auc_score(y_test, y_pred)\n",
        "print('Test set AUC of rf with max_depth_12 : {:.4f}'.format(auc_rf_12))\n",
        "#Optimal depth is at 12, gives us 93.5% accuracy\n",
        "#But, 6 gives 92.5% accuracy. So 6 is more efficient\n",
        "#seems like Ada gives the best AUC and rf gives the best accuracy. Can we use ada with rf?\n",
        "\n",
        "# rf_accuracy = []\n",
        "# for i in range(1,12):\n",
        "#     rf_iterated = RandomForestClassifier(max_depth=i, random_state = 0)\n",
        "#     rf_iterated.fit(X_train, y_train)\n",
        "#     y_pred = rf_iterated.predict(X_test)\n",
        "#     acc_test = accuracy_score(y_test, y_pred)\n",
        "#     auc_rf_iterated = roc_auc_score(y_test, y_pred)\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': range(1,100),\n",
        "    'max_features': ['log2', 'auto', 'sqrt'],\n",
        "    'min_samples_leaf': [2, 10],\n",
        "    'max_depth': [2, 12]}\n",
        "rf_cvgrid = RandomForestClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator = rf_cvgrid, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 1, scoring = 'roc_auc')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#evaluate accuracy\n",
        "y_pred_opt = grid_search.predict(X_test)\n",
        "rf_opt_acc = accuracy_score(y_test, y_pred_opt)\n",
        "print('Test set accuraacy of rf optimized: {:.3f}'.format(rf_opt_acc))\n",
        "\n",
        "#evaluate ROC_AUC score\n",
        "auc_rf_opt = roc_auc_score(y_test, y_pred_opt)\n",
        "print('Test set AUC of rf_opt : {:.4f}'.format(auc_rf_opt))\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Final model\n",
        "\n",
        "def predictor_model(obs):\n",
        "    #input must be array of 1,15\n",
        "    obs_reshaped = np.array(obs).reshape(1,15)\n",
        "    return obs_reshaped\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
